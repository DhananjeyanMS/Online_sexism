{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYV6mzNka47M",
        "outputId": "be121e01-ebab-4fe2-fdf5-a4e1b2f66e16"
      },
      "id": "OYV6mzNka47M",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers==4.28.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWz4bOB_bIu3",
        "outputId": "27325b06-037c-4093-8551-9e8e097ffa7a"
      },
      "id": "fWz4bOB_bIu3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.28.0\n",
            "  Downloading transformers-4.28.0-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.28.0)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b504a39",
      "metadata": {
        "id": "7b504a39"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import cuda\n",
        "import torch\n",
        "from transformers import DistilBertConfig\n",
        "\n",
        "# Load and preprocess the data\n",
        "data = pd.read_csv('/content/edos_labelled_aggregated.csv')\n",
        "labels = data['label_sexist'].values\n",
        "categories = data['label_category'].values\n",
        "vectors = data['label_vector'].values\n",
        "texts = data['text'].values\n",
        "\n",
        "# Load and preprocess the data\n",
        "#data = pd.read_csv('/content/edos_labelled_aggregated.csv')\n",
        "#train_labels = data['label_sexist'].values\n",
        "#train_categories = data['label_category'].values\n",
        "#train_vectors = data['label_vector'].values\n",
        "#train_texts = data['text'].values\n",
        "\n",
        "# Load and preprocess the data\n",
        "#test = pd.read_csv('/content/test.csv')\n",
        "#test_labels = data['label_sexist'].values\n",
        "#test_categories = data['label_category'].values\n",
        "#test_vectors = data['label_vector'].values\n",
        "#test_texts = data['text'].values\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_texts, test_texts, train_labels, test_labels, train_categories, test_categories, train_vectors, test_vectors = train_test_split(\n",
        "    texts, labels, categories, vectors, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define the label mapping\n",
        "label_mapping = {\n",
        "    'sexist': 1,\n",
        "    'not sexist': 0\n",
        "}\n",
        "\n",
        "category_mapping = {\n",
        "    'none': 0,\n",
        "    '1. threats, plans to harm and incitement': 1,\n",
        "    '2. derogation': 2,\n",
        "    '3. animosity': 3,\n",
        "    '4. prejudiced discussions': 4\n",
        "\n",
        "    # Add more categories as necessary\n",
        "}\n",
        "\n",
        "vector_mapping = {\n",
        "    'none': 0,\n",
        "    '1.1 threats of harm': 1,\n",
        "    '1.2 incitement and encouragement of harm': 2,\n",
        "    '2.1 descriptive attacks': 3,\n",
        "    '2.2 aggressive and emotive attacks': 4,\n",
        "    '2.3 dehumanising attacks & overt sexual objectification': 5,\n",
        "    '3.1 casual use of gendered slurs, profanities, and insults': 6,\n",
        "    '3.2 immutable gender differences and gender stereotypes': 7,\n",
        "    '3.3 backhanded gendered compliments': 8,\n",
        "    '3.4 condescending explanations or unwelcome advice': 9,\n",
        "    '4.1 supporting mistreatment of individual women': 10,\n",
        "    '4.2 supporting systemic discrimination against women as a group': 11\n",
        "    \n",
        "\n",
        "    # Add more categories as necessary\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1653cfa",
      "metadata": {
        "id": "e1653cfa"
      },
      "outputs": [],
      "source": [
        "# Define the custom dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, categories, vectors, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.categories = categories\n",
        "        self.vectors = vectors\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        category = self.categories[idx]\n",
        "        vector = self.vectors[idx]\n",
        "    \n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "    \n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label_mapping[label]),  # Encode label as numerical value\n",
        "            'category': torch.tensor(category_mapping[category]),  # Encode category as numerical value\n",
        "            'vector': torch.tensor(vector_mapping[vector])\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de3e7486",
      "metadata": {
        "id": "de3e7486"
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# Set hyperparameters\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertModel\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomDistilBertForSequenceClassification(DistilBertForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.distilbert = DistilBertModel(config)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.classifier = nn.Linear(config.hidden_size + 1 + 1 + 1 , config.num_labels)  # Include 1 additional unit for each extra feature\n",
        "    \n",
        "    def forward(self, input_ids=None, attention_mask=None, category=None, labels=None, vector=None, **kwargs):\n",
        "        distilbert_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
        "        hidden_state = distilbert_output.last_hidden_state[:, 0, :]  # Extract the [CLS] token embedding\n",
        "        hidden_state = self.dropout(hidden_state)\n",
        "    \n",
        "        # Reshape the category tensor to match the dimensions of the hidden_state tensor\n",
        "        if category is not None:\n",
        "            category = category.unsqueeze(1)  # Add an extra dimension\n",
        "        \n",
        "        # Reshape the labels tensor to match the dimensions of the hidden_state tensor\n",
        "        if labels is not None:\n",
        "            labels = labels.unsqueeze(1)  # Add an extra dimension\n",
        "    \n",
        "        # Reshape the vector tensor to match the dimensions of the hidden_state tensor\n",
        "        if vector is not None:\n",
        "            vector = vector.unsqueeze(1)  # Add an extra dimension\n",
        "    \n",
        "        # Concatenate the hidden state with the extra features\n",
        "        if category is not None:\n",
        "            hidden_state = torch.cat((hidden_state, category), dim=1)\n",
        "        if labels is not None:\n",
        "            hidden_state = torch.cat((hidden_state, labels), dim=1)\n",
        "        if vector is not None:\n",
        "            hidden_state = torch.cat((hidden_state, vector), dim=1)\n",
        "    \n",
        "        logits = self.classifier(hidden_state)\n",
        "        outputs = (logits,) + distilbert_output[1:]  # Add hidden states and attention if they are present\n",
        "    \n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "139faa34",
      "metadata": {
        "id": "139faa34",
        "outputId": "965c3cea-4e0e-449e-8da3-ce350eb7a145",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Average Loss: 0.4462\n",
            "Overall acore\n",
            "Accuracy: 0.8413\n",
            "Precision: 0.8053\n",
            "Recall: 0.4840\n",
            "F1-score: 0.6047\n",
            "---\n",
            "Test Loss: 0.0\n",
            "\n",
            "Scores for Vectors:\n",
            "Accuracy: 0.4885\n",
            "Precision: 0.4885\n",
            "Recall: 0.4885\n",
            "F1: 0.4885\n",
            "\n",
            "Scores for Labels:\n",
            "Accuracy: 0.8458\n",
            "Precision: 0.8458\n",
            "Recall: 0.8458\n",
            "F1: 0.8458\n",
            "\n",
            "Scores for Categories:\n",
            "Accuracy: 0.4973\n",
            "Precision: 0.4973\n",
            "Recall: 0.4973\n",
            "F1: 0.4973\n",
            "Epoch 2/5, Average Loss: 0.3119\n",
            "Overall acore\n",
            "Accuracy: 0.8573\n",
            "Precision: 0.7700\n",
            "Recall: 0.6144\n",
            "F1-score: 0.6834\n",
            "---\n",
            "Test Loss: 0.0\n",
            "\n",
            "Scores for Vectors:\n",
            "Accuracy: 0.6068\n",
            "Precision: 0.6068\n",
            "Recall: 0.6068\n",
            "F1: 0.6068\n",
            "\n",
            "Scores for Labels:\n",
            "Accuracy: 0.8560\n",
            "Precision: 0.8560\n",
            "Recall: 0.8560\n",
            "F1: 0.8560\n",
            "\n",
            "Scores for Categories:\n",
            "Accuracy: 0.6068\n",
            "Precision: 0.6068\n",
            "Recall: 0.6068\n",
            "F1: 0.6068\n",
            "Epoch 3/5, Average Loss: 0.2369\n",
            "Overall acore\n",
            "Accuracy: 0.8553\n",
            "Precision: 0.7751\n",
            "Recall: 0.5957\n",
            "F1-score: 0.6737\n",
            "---\n",
            "Test Loss: 0.0\n",
            "\n",
            "Scores for Vectors:\n",
            "Accuracy: 0.6635\n",
            "Precision: 0.6635\n",
            "Recall: 0.6635\n",
            "F1: 0.6635\n",
            "\n",
            "Scores for Labels:\n",
            "Accuracy: 0.8598\n",
            "Precision: 0.8598\n",
            "Recall: 0.8598\n",
            "F1: 0.8598\n",
            "\n",
            "Scores for Categories:\n",
            "Accuracy: 0.6735\n",
            "Precision: 0.6735\n",
            "Recall: 0.6735\n",
            "F1: 0.6735\n",
            "Epoch 4/5, Average Loss: 0.1804\n",
            "Overall acore\n",
            "Accuracy: 0.8360\n",
            "Precision: 0.6675\n",
            "Recall: 0.6888\n",
            "F1-score: 0.6780\n",
            "---\n",
            "Test Loss: 0.0\n",
            "\n",
            "Scores for Vectors:\n",
            "Accuracy: 0.5200\n",
            "Precision: 0.5200\n",
            "Recall: 0.5200\n",
            "F1: 0.5200\n",
            "\n",
            "Scores for Labels:\n",
            "Accuracy: 0.8403\n",
            "Precision: 0.8403\n",
            "Recall: 0.8403\n",
            "F1: 0.8403\n",
            "\n",
            "Scores for Categories:\n",
            "Accuracy: 0.5200\n",
            "Precision: 0.5200\n",
            "Recall: 0.5200\n",
            "F1: 0.5200\n",
            "Epoch 5/5, Average Loss: 0.1337\n",
            "Overall acore\n",
            "Accuracy: 0.8493\n",
            "Precision: 0.6884\n",
            "Recall: 0.7287\n",
            "F1-score: 0.7080\n",
            "---\n",
            "Test Loss: 0.0\n",
            "\n",
            "Scores for Vectors:\n",
            "Accuracy: 0.6045\n",
            "Precision: 0.6045\n",
            "Recall: 0.6045\n",
            "F1: 0.6045\n",
            "\n",
            "Scores for Labels:\n",
            "Accuracy: 0.8508\n",
            "Precision: 0.8508\n",
            "Recall: 0.8508\n",
            "F1: 0.8508\n",
            "\n",
            "Scores for Categories:\n",
            "Accuracy: 0.6215\n",
            "Precision: 0.6215\n",
            "Recall: 0.6215\n",
            "F1: 0.6215\n"
          ]
        }
      ],
      "source": [
        "# Load the tokenizer and model\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "config = DistilBertConfig.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
        "model = CustomDistilBertForSequenceClassification(config)\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "# Create data loaders\n",
        "train_dataset = TextDataset(train_texts, train_labels, train_categories, train_vectors, tokenizer, MAX_LEN)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "test_dataset = TextDataset(test_texts, test_labels, test_categories, test_vectors, tokenizer, MAX_LEN)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Set optimizer and scheduler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        categories = batch['category'].to(device)\n",
        "        vectors = batch['vector'].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "            category=categories,\n",
        "            vector=vectors\n",
        "        )\n",
        "        \n",
        "        logits = outputs[0]\n",
        "        loss = criterion(logits, labels)  # Calculate the loss\n",
        "        \n",
        "               \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch+1}/{EPOCHS}, Average Loss: {avg_loss:.4f}')\n",
        "    \n",
        "    # Evaluation on the test set\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    test_loss = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    true_categories = []\n",
        "    true_vectors = []\n",
        "    predicted_categories = []\n",
        "    predicted_labels = []\n",
        "    category_scores = {}  # Dictionary to store accuracy for each category\n",
        "    label_scores = {}\n",
        "    vector_scores = {}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            category = batch['category'].to(device)\n",
        "            vector = batch['vector'].to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels,\n",
        "                category=category,\n",
        "                vector=vector\n",
        "            )\n",
        "            \n",
        "            logits = outputs[0]\n",
        "            softmax_probs = torch.softmax(logits, dim=1)\n",
        "            predicted_labels_batch = torch.argmax(softmax_probs, dim=1)\n",
        "        \n",
        "            predictions.extend(predicted_labels_batch.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "            \n",
        "            predicted_labels.extend(predicted_labels_batch.cpu().numpy())\n",
        "            true_vectors.extend(vectors.cpu().numpy())\n",
        "\n",
        "            true_categories.extend(categories.cpu().numpy())\n",
        "            predicted_categories.extend(predicted_labels_batch.cpu().numpy())\n",
        "            \n",
        "            \n",
        "            # Calculate precision, recall, and F1-score for each label\n",
        "            for i in range(len(predicted_labels_batch)):\n",
        "                true_label = labels[i].item()\n",
        "                predicted_label = predicted_labels_batch[i].item()\n",
        "                sexist_label = test_labels[i]\n",
        "            \n",
        "                if sexist_label not in label_scores:\n",
        "                    label_scores[sexist_label] = {'true_positive': 0, 'false_positive': 0, 'false_negative': 0, 'true_negative': 0}\n",
        "            \n",
        "                if true_label == predicted_label == 1:\n",
        "                    label_scores[sexist_label]['true_positive'] += 1\n",
        "                elif true_label == 0 and predicted_label == 1:\n",
        "                    label_scores[sexist_label]['false_positive'] += 1\n",
        "                elif true_label == 1 and predicted_label == 0:\n",
        "                    label_scores[sexist_label]['false_negative'] += 1\n",
        "                elif true_label == 0 and predicted_label == 0:\n",
        "                    label_scores[sexist_label]['true_negative'] += 1\n",
        "         \n",
        "    for sexist_label, scores in label_scores.items():\n",
        "        true_positive = scores['true_positive']\n",
        "        false_positive = scores['false_positive']\n",
        "        false_negative = scores['false_negative']\n",
        "        true_negative = scores['true_negative']\n",
        "    \n",
        "        label_accuracy = (true_positive + true_negative)/(true_positive + true_negative + false_positive + false_negative)\n",
        "        label_precision = true_positive / (true_positive + false_positive + 1e-10)\n",
        "        label_recall = true_positive / (true_positive + false_negative + 1e-10)\n",
        "        label_f1 = 2 * (label_precision * label_recall) / (label_precision + label_recall + 1e-10)\n",
        "    \n",
        "    print('Overall acore')\n",
        "    print(f'Accuracy: {label_accuracy:.4f}')\n",
        "    print(f'Precision: {label_precision:.4f}')\n",
        "    print(f'Recall: {label_recall:.4f}')\n",
        "    print(f'F1-score: {label_f1:.4f}')\n",
        "    print('---')\n",
        "\n",
        "    avg_test_loss = test_loss / len(test_loader)\n",
        "      \n",
        "    # Calculate cumulative scores\n",
        "    print(f'Test Loss: {avg_test_loss}')\n",
        "\n",
        "    \n",
        "    # Calculate cumulative scores for vectors, labels, and categories\n",
        "    #cumulative_true_positive = sum([score['true_positive'] for score in category_scores.values()])\n",
        "    #cumulative_false_positive = sum([score['false_positive'] for score in category_scores.values()])\n",
        "    #cumulative_false_negative = sum([score['false_negative'] for score in category_scores.values()])\n",
        "\n",
        "    #cumulative_precision = cumulative_true_positive / (cumulative_true_positive + cumulative_false_positive + 1e-7)\n",
        "    #cumulative_recall = cumulative_true_positive / (cumulative_true_positive + cumulative_false_negative + 1e-7)\n",
        "    #cumulative_accuracy = (cumulative_true_positive + len(test_loader) - cumulative_false_positive - cumulative_false_negative) / len(test_loader)\n",
        "\n",
        "    vector_accuracy = accuracy_score(true_vectors, predicted_labels)\n",
        "    vector_precision = precision_score(true_vectors, predicted_labels, average='micro')\n",
        "    vector_recall = recall_score(true_vectors, predicted_labels, average='micro')\n",
        "    vector_f = f1_score(true_vectors, predicted_labels, average='micro')\n",
        "\n",
        "    label_accuracy = accuracy_score(true_labels, predictions)\n",
        "    label_precision = precision_score(true_labels, predictions, average='micro')\n",
        "    label_recall = recall_score(true_labels, predictions, average='micro')\n",
        "    label_f = f1_score(true_labels, predictions, average='micro')\n",
        "\n",
        "    category_accuracy = accuracy_score(true_categories, predicted_categories)\n",
        "    category_precision = precision_score(true_categories, predicted_categories, average='micro')\n",
        "    category_recall = recall_score(true_categories, predicted_categories, average='micro')\n",
        "    category_f = f1_score(true_categories, predicted_categories, average='micro')\n",
        "\n",
        "   \n",
        "\n",
        "    print(\"\\nScores for Vectors:\")\n",
        "    print(f\"Accuracy: {vector_accuracy:.4f}\")\n",
        "    print(f\"Precision: {vector_precision:.4f}\")\n",
        "    print(f\"Recall: {vector_recall:.4f}\")\n",
        "    print(f\"F1: {vector_f:.4f}\")\n",
        "\n",
        "    print(\"\\nScores for Labels:\")\n",
        "    print(f\"Accuracy: {label_accuracy:.4f}\")\n",
        "    print(f\"Precision: {label_precision:.4f}\")\n",
        "    print(f\"Recall: {label_recall:.4f}\")\n",
        "    print(f\"F1: {label_f:.4f}\")\n",
        "\n",
        "    print(\"\\nScores for Categories:\")\n",
        "    print(f\"Accuracy: {category_accuracy:.4f}\")\n",
        "    print(f\"Precision: {category_precision:.4f}\")\n",
        "    print(f\"Recall: {category_recall:.4f}\")\n",
        "    print(f\"F1: {category_f:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0d513b2",
      "metadata": {
        "id": "a0d513b2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}